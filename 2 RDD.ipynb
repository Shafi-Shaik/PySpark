{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a104ec82-dfad-48ea-a855-c34f0f6b1344",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b18d6a6-9408-4732-971c-f17fc541eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#SparkSession\n",
    "spark=SparkSession.builder.master('local').appName('practice session 1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94a233f-f99b-4205-baf8-7091dc67b50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ShafiVivobook.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice session 1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2572ce52740>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3db728-c5dc-471d-bc4d-8e79dc9ce5af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.RDD using parallelize() for Random/adhoc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768bce23-eedd-49b4-88fa-95c8da3de4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize((1,2,3,4,5,6,7,8,9,10)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a4445e-97ef-4ed3-ae39-e190e07b0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=spark.sparkContext.parallelize([('Ross',13),('shafi',21),('Ravi',19),('Ravi',34),('shaik',123),('tommy',89),('jack',35),('splash',1.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32d75f24-61b8-46e9-9466-af7ea83cc4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ross', 13),\n",
       " ('shafi', 21),\n",
       " ('Ravi', 19),\n",
       " ('Ravi', 34),\n",
       " ('shaik', 123),\n",
       " ('tommy', 89)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d67af4-1680-4985-9920-42236cd4e746",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. textFile() : \n",
    "<h4> Read single or multiple text, csv files and returns a single Spark RDD [String] </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33717fb6-ca02-4e1f-b911-e7c02bdc074c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Read single text files into a single RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddf7737-16ca-4fe0-833f-7da9a7cfe68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_rdd=spark.sparkContext.textFile('S:/DataSets/text/t1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e6acc2-2af9-45f7-842f-94e3edf0299d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de663d6-5ae4-4a3c-9b66-bf9432ba8664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t1, file, 1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af917403-6d3b-4353-9e69-092c4deaaea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Read multiple text files into a single RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bf3e2f-bb3a-43a8-9051-ef3466b7db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_2 = spark.sparkContext.textFile(\"S:/DataSets/text/t1.txt,S:/DataSets/text/t2.txt,S:/DataSets/text/t3.txt,S:/DataSets/text/t4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e003c3-2632-4b4d-a3dd-d4260c8996c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t1, file, 1', 't2,file,2', 't3,file,3', 't4,4,file']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first collect the and then loop thrugh t\n",
    "users_2Collect=users_2.collect()\n",
    "users_2Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d022f7-cdf4-4284-929e-1fc39ccc352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1, file, 1\n",
      "t2,file,2\n",
      "t3,file,3\n",
      "t4,4,file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## looping through the collect\n",
    "for line in users_2Collect:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3be95-fce4-493a-a39b-1ffbf38c7444",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read all text files matching a pattern to single RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98f090a-1548-4417-b109-9b50aead9ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S:/Datasets/text/t*.txt MapPartitionsRDD[9] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh=spark.sparkContext.textFile(\"S:/Datasets/text/t*.txt\")\n",
    "sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0becde25-dd54-4ee9-ba63-2c59f85e3e3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sh.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0aefb-6023-4756-98b9-cb2737f0d1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02b32035-7085-468d-acc3-1118330f2328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## with partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2fc9c23-8bba-4105-ac89-316cf7fcac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3=spark.sparkContext.textFile(\"S:\\\\Datasets\\\\text\\\\word_list_moby.txt\",minPartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a94fd99-8a9f-4327-aaec-b2439437b0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f69688b-b91d-4aa7-986d-1870ee278b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg Etext of Moby Word II by Grady Ward',\n",
       " '',\n",
       " 'Copyright laws are changing all over the world, be sure to check',\n",
       " 'the laws for your country before redistributing these files!!!',\n",
       " '']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c6e97-a097-40e9-8750-a599d33746df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. wholeTextFiles() :\n",
    "\n",
    "<h4> Reads single or multiple files and returns a single RDD[Tuple2[String, String]], where first value (_1) in a tuple is a file name and second value (_2) is content of the file. </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c75ff0-f984-463c-b3e9-3865ae3fb684",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read a single text file into single RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66f695e8-fd83-46a8-9fcc-2d2f1f25c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh=spark.sparkContext.wholeTextFiles(\"S:/Datasets/text/t1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7426216c-380c-46fb-babb-ccef1c779d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/S:/Datasets/text/t1.txt', 't1, file, 1')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shc=sh.collect()\n",
    "shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b0185c-bb05-46a0-bd75-c5e6e75fb09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('file:/S:/Datasets/text/t1.txt', 't1, file, 1')\n"
     ]
    }
   ],
   "source": [
    "for i in shc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156394a-5496-4704-8a16-050f394d8622",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read Multiple text files into single RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20c652f5-5755-4c72-9e29-ace7a510123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh=spark.sparkContext.wholeTextFiles(\"S:/Datasets/text/t1.txt,S:/Datasets/text/t2.txt,S:/Datasets/text/t3.txt,S:/Datasets/text/t4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4653503-8e07-426f-a99c-0a0b581b4eb2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/S:/Datasets/text/t1.txt', 't1, file, 1'),\n",
       " ('file:/S:/Datasets/text/t2.txt', 't2,file,2'),\n",
       " ('file:/S:/Datasets/text/t3.txt', 't3,file,3'),\n",
       " ('file:/S:/Datasets/text/t4.txt', 't4,4,file')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shc=sh.collect()\n",
    "shc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e33c9180-e819-42dd-9fda-e3bf083ca57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('file:/S:/Datasets/text/t1.txt', 't1, file, 1')\n",
      "('file:/S:/Datasets/text/t2.txt', 't2,file,2')\n",
      "('file:/S:/Datasets/text/t3.txt', 't3,file,3')\n",
      "('file:/S:/Datasets/text/t4.txt', 't4,4,file')\n"
     ]
    }
   ],
   "source": [
    "for i in shc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8c583-e1d3-4cef-8613-fd37e59f15cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read all text files with pattern match into single RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a22fea8e-0702-485d-8abe-6bfdbf4cb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh1=spark.sparkContext.wholeTextFiles(\"S:/Datasets/text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "112450c7-4dec-4547-bf4f-a49927fccffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sh1c=sh.collect()\n",
    "#sh1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca6240ff-a841-4196-80bc-9d71e05c3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in sh1c:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a03c4-a58a-4527-8905-8807ecbaa179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Create empty RDD using sparkContext.emptyRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29858ff0-0a99-4a15-b763-1b2e0ff602ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n",
    "\n",
    "\n",
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d1608ee-49ef-41b9-9492-d9c6478d58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b241355-f2d0-4910-b3fe-3a07b041b221",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Repartition and Coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f54ed-a627-4e01-853c-0f256ce29559",
   "metadata": {},
   "source": [
    "Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition; \n",
    " 1. first using repartition() method which shuffles data from all nodes also called full shuffle and\n",
    " 2. second coalesce() method which shuffle data from minimum nodes,\n",
    "\n",
    "for examples if  you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.  \n",
    "\n",
    "Both of the functions take the number of partitions to repartition rdd as shown below.  Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87459015-e17c-4a2c-ba8c-52c0b6ad566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-partition count:4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reparRdd = rdd3.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))\n",
    "#Outputs: \"re-partition count:4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a47774-5a32-4de4-99b5-c241cb12684e",
   "metadata": {},
   "source": [
    "###### repartition() or coalesce() methods also returns a new RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd4b26-2004-40d5-9d52-a62ab0a7ca27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RDD Transformations\n",
    "* Transformations are lazy operations, instead of updating an RDD, these operations return another RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c91088f-9667-4119-a7a6-636134e92835",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = (\"Project Gutenberg’s\",\"Alice’s Adventures in Wonderland\", \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\"Project Gutenberg’s\")\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "#Create RDD from external Data source\n",
    "#rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")\n",
    "\n",
    "for element in rdd.collect():\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138d9f2a-8e26-4d55-8bdf-ea23d542116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project', 'Gutenberg’s'],\n",
       " ['Alice’s', 'Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenberg’s'],\n",
       " ['Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenberg’s']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                               #map   \n",
    "    \n",
    "rdd2=rdd.map(lambda x: x.split(\" \"))\n",
    "# for element in rdd2.collect():\n",
    "#     print(element)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7cbfa0d-a067-49cf-8eae-7ea7a92d4271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                                #Flatmap   \n",
    "    \n",
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "# for element in rdd2.collect():\n",
    "#     print(element)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652b927d-d3d4-456e-8618-a1480e1a1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "                                                #map\n",
    "rdd3=rdd2.map(lambda x: (x,1))\n",
    "for element in rdd3.collect():\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc23661d-35db-4300-b2cf-ddfd3c46a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 3)\n",
      "('Gutenberg’s', 3)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 2)\n",
      "('in', 2)\n",
      "('Wonderland', 2)\n"
     ]
    }
   ],
   "source": [
    "                                                #reduceByKey\n",
    "rdd4=rdd3.reduceByKey(lambda a,b: a+b)\n",
    "for element in rdd4.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "747f2301-e4f4-4d1e-bb96-233810e3b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice’s')\n",
      "(2, 'Adventures')\n",
      "(2, 'in')\n",
      "(2, 'Wonderland')\n",
      "(3, 'Project')\n",
      "(3, 'Gutenberg’s')\n"
     ]
    }
   ],
   "source": [
    "                                                #sortByKey\n",
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "for element in rdd5.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cc39304-25eb-4afb-ad58-d057e9bc7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'Wonderland')\n"
     ]
    }
   ],
   "source": [
    "                                            #filter\n",
    "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
    "for element in rdd6.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec7bd7-89ad-44c9-9dc4-7e5f4e3c799d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RDD Actions \n",
    "* operations that trigger computation and return RDD values.\n",
    "* return the values from an RDD to a driver program. In other words, any RDD function that returns non-RDD is considered as an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "760032c5-91d2-4f8a-860d-59eb62e5f8aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd6 =  [(2, 'Wonderland')]\n",
      "Count : 1\n",
      "First Record : 2,Wonderland\n",
      "Max Record : 2,Wonderland\n",
      "dataReduce Record : 2\n",
      "data3 Key:2, Value:Wonderland\n"
     ]
    }
   ],
   "source": [
    "print(\"rdd6 = \",rdd6.collect())\n",
    "# Action - count\n",
    "print(\"Count : \"+str(rdd6.count()))\n",
    "\n",
    "\n",
    "# Action - first\n",
    "firstRec = rdd6.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])\n",
    "\n",
    "\n",
    "# Action - max\n",
    "datMax = rdd6.max()\n",
    "print(\"Max Record : \"+str(datMax[0]) + \",\"+ datMax[1])\n",
    "\n",
    "\n",
    "# Action - reduce\n",
    "totalWordCount = rdd6.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))\n",
    "\n",
    "\n",
    "# Action - take\n",
    "data3 = rdd6.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7562c816-23ce-460d-bf8b-978f67c71c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 3, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
    "inputRDD = spark.sparkContext.parallelize(data)\n",
    "  \n",
    "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n",
    "\n",
    "listRdd.collect() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b165ea5-ee67-4e5b-9baf-327c77fc5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(20, 7)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#aggregate - Aggregate the elements of each partition, and then the results for all the partitions, \n",
    "#            using a given combine functions “combOp” and a neutral “zero value.”\n",
    "\n",
    "seqOp = (lambda x, y: x + y) #partition aggregate function\n",
    "combOp = (lambda x, y: x + y) #combine aggregate function\n",
    "\n",
    "agg=listRdd.aggregate(0, seqOp, combOp)\n",
    "\n",
    "print(agg) # output 20\n",
    "\n",
    "#aggregate 2\n",
    "seqOp2 = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp2 = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg2=listRdd.aggregate((0, 0), seqOp2, combOp2)\n",
    "\n",
    "print(agg2) # output (20,7)\n",
    "\n",
    "agg2=listRdd.treeAggregate(0,seqOp, combOp)\n",
    "print(agg2) # output 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10b9f950-5bbe-4618-a9b1-b42299a64be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#fold - Aggregate the elements of each partition, and then the results for all the partitions\n",
    "from operator import add\n",
    "foldRes=listRdd.fold(0, add)\n",
    "print(foldRes) # output 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "373c9896-661f-4e6d-8c4d-ddcf876aaf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition :  20\n",
      "Multiplication :  720\n"
     ]
    }
   ],
   "source": [
    "#reduce - Reduces the elements of the dataset using the specified binary operator.\n",
    "\n",
    "from operator import mul,add\n",
    "\n",
    "redRes=listRdd.reduce(add)\n",
    "print('Addition : ' ,redRes) # output 20\n",
    "\n",
    "redRes=listRdd.reduce(mul)\n",
    "print('Multiplication : ',redRes) # output 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4145f76c-cc2b-4118-bfb7-9832587d675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#treeReduce. This is similar to reduce. Reduces the elements of this RDD in a multi-level tree pattern\n",
    "\n",
    "add = lambda x, y: x + y\n",
    "redRes=listRdd.treeReduce(add)\n",
    "print(redRes) # output 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56628a8f-610c-42d2-81d1-88c2553f7f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "#Collect - Return the complete dataset as an Array.\n",
    "data = listRdd.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6753ae47-cfed-4cf8-a731-78a327ca3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 7\n",
      "countApprox : 7\n",
      "countApproxDistinct : 5\n",
      "countByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "#count, countApprox, countApproxDistinct\n",
    "\n",
    "print(\"Count : \"+str(listRdd.count()))\n",
    "#Output: Count : 20\n",
    "\n",
    "print(\"countApprox : \"+str(listRdd.countApprox(1200)))\n",
    "#Output: countApprox : (final: [7.000, 7.000])\n",
    "\n",
    "print(\"countApproxDistinct : \"+str(listRdd.countApproxDistinct()))\n",
    "#Output: countApproxDistinct : 5\n",
    "\n",
    "#countByValue, countByValueApprox\n",
    "print(\"countByValue :  \"+str(listRdd.countByValue()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04d5aed1-bb1d-4972-85c0-38ae04a4714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first :  1\n",
      "first :  ('Z', 1)\n",
      "top : [5, 4]\n",
      "top : [('Z', 1), ('C', 40)]\n"
     ]
    }
   ],
   "source": [
    "#first – Return the first element in the dataset.\n",
    "print(\"first :  \"+str(listRdd.first()))\n",
    "#Output: first :  1\n",
    "print(\"first :  \"+str(inputRDD.first()))\n",
    "#Output: first :  (Z,1)\n",
    "\n",
    "\n",
    "\n",
    "#top– Return top n elements from the dataset.\n",
    "print(\"top : \"+str(listRdd.top(2)))\n",
    "#Output: take : 5,4\n",
    "print(\"top : \"+str(inputRDD.top(2)))\n",
    "#Output: take : (Z,1),(C,40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20eb275e-57e5-4f43-b253-2747ef219c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min :  1\n",
      "min :  ('A', 20)\n",
      "max :  5\n",
      "max :  ('Z', 1)\n"
     ]
    }
   ],
   "source": [
    "#min\n",
    "print(\"min :  \"+str(listRdd.min()))\n",
    "#Output: min :  1\n",
    "print(\"min :  \"+str(inputRDD.min()))\n",
    "#Output: min :  (A,20)  \n",
    "\n",
    "#max\n",
    "print(\"max :  \"+str(listRdd.max()))\n",
    "#Output: max :  5\n",
    "print(\"max :  \"+str(inputRDD.max()))\n",
    "#Output: max :  (Z,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bc28d62-db32-4674-a0a7-5bc39fc1bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "take : [1, 2]\n",
      "takeOrdered : [1, 2]\n"
     ]
    }
   ],
   "source": [
    "#take, takeOrdered, takeSample\n",
    "print(\"take : \"+str(listRdd.take(2)))\n",
    "#Output: take : 1,2\n",
    "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))\n",
    "#Output: takeOrdered : 1,2\n",
    "\n",
    "\n",
    "#print(\"take : \"+str(listRdd.takeSample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa94c1-27e4-4fe7-9776-837ae40dc6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58458c67-c6bd-439b-95c7-2a7de2e8832b",
   "metadata": {},
   "source": [
    "Refer this page for more info on RDD : https://sparkbyexamples.com/pyspark-rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16000ea-821c-474c-b27c-666fe0aa45c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6aa73a-2827-4e0a-85d6-efd070250a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
